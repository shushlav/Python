{
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# Perceptron Basics"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "source": [
                "x_input = [0.1, 0.5, 0.2]    \r\n",
                "w_weights =  [0.4, 0.3, 0.6]    # בחירת המשקולות\r\n",
                "threshold = 0.5                 # bias\r\n",
                "\r\n",
                "def step(weighted_sum):\r\n",
                "    if weighted_sum > threshold:\r\n",
                "        return 1\r\n",
                "    else:\r\n",
                "        return 0\r\n",
                "\r\n",
                "def perceptron():\r\n",
                "    weighted_sum = 0\r\n",
                "    for x,w in zip(x_input, w_weights):      # iterates untill finish\r\n",
                "       weighted_sum += x*w\r\n",
                "       print(weighted_sum)\r\n",
                "    return step(weighted_sum)\r\n",
                "\r\n",
                "output = perceptron()\r\n",
                "print(\"output: \", str(output))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "0.04000000000000001\n",
                        "0.19\n",
                        "0.31\n",
                        "output:  0\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "because the sum (0.31) was smaller than our threshold (0.5) the output is 0."
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "source": [
                "import numpy as np\r\n",
                "# Setting the random seed, feel free to change it and see different solutions.\r\n",
                "np.random.seed(42)\r\n",
                "\r\n",
                "def stepFunction(t):\r\n",
                "    if t >= 0:\r\n",
                "        return 1\r\n",
                "    return 0\r\n",
                "\r\n",
                "def prediction(X, W, b):\r\n",
                "    return stepFunction((np.matmul(X,W)+b)[0])\r\n",
                "\r\n",
                "# TODO: Fill in the code below to implement the perceptron trick.\r\n",
                "# The function should receive as inputs the data X, the labels y,\r\n",
                "# the weights W (as an array), and the bias b,\r\n",
                "# update the weights and bias W, b, according to the perceptron algorithm,\r\n",
                "# and return W and b.\r\n",
                "def perceptronStep(X, y, W, b, learn_rate = 0.01):\r\n",
                "    for i in range(len(X)):\r\n",
                "        y_hat = prediction(X[i],W,b)\r\n",
                "        if y[i]-y_hat == 1:\r\n",
                "            W[0] += X[i][0]*learn_rate\r\n",
                "            W[1] += X[i][1]*learn_rate\r\n",
                "            b += learn_rate\r\n",
                "        elif y[i]-y_hat == -1:\r\n",
                "            W[0] -= X[i][0]*learn_rate\r\n",
                "            W[1] -= X[i][1]*learn_rate\r\n",
                "            b -= learn_rate\r\n",
                "    return W, b\r\n",
                "    \r\n",
                "# This function runs the perceptron algorithm repeatedly on the dataset,\r\n",
                "# and returns a few of the boundary lines obtained in the iterations,\r\n",
                "# for plotting purposes.\r\n",
                "# Feel free to play with the learning rate and the num_epochs,\r\n",
                "# and see your results plotted below.\r\n",
                "def trainPerceptronAlgorithm(X, y, learn_rate = 0.01, num_epochs = 25):\r\n",
                "    x_min, x_max = min(X.T[0]), max(X.T[0])\r\n",
                "    y_min, y_max = min(X.T[1]), max(X.T[1])\r\n",
                "    W = np.array(np.random.rand(2,1))\r\n",
                "    b = np.random.rand(1)[0] + x_max\r\n",
                "    # These are the solution lines that get plotted below.\r\n",
                "    boundary_lines = []\r\n",
                "    for i in range(num_epochs):\r\n",
                "        # In each epoch, we apply the perceptron step.\r\n",
                "        W, b = perceptronStep(X, y, W, b, learn_rate)\r\n",
                "        boundary_lines.append((-W[0]/W[1], -b/W[1]))\r\n",
                "    return boundary_lines\r\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "Write a function that takes as input a list of numbers, and returns the list of values given by the softmax function."
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "source": [
                "import numpy as np\r\n",
                "\r\n",
                "def softmax(L):\r\n",
                "    expL = np.exp(L)       # e = 2.71828 np has specific function\r\n",
                "    sumExpL = sum(expL)\r\n",
                "    result = []\r\n",
                "    for i in expL:\r\n",
                "        result.append(i*1.0/sumExpL)\r\n",
                "    return result\r\n",
                "softmax([2,1,0])    "
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "[0.6652409557748219, 0.24472847105479764, 0.09003057317038046]"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 4
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Cross-entropy\r\n",
                "Let's code the formula for cross-entropy in Python.  Y is for the category, and P is the probability."
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "source": [
                "import numpy as np\r\n",
                "def cross_entropy(Y, P):\r\n",
                "    Y = np.float_(Y)                    \r\n",
                "    P = np.float_(P)\r\n",
                "    print(Y,P)\r\n",
                "    return (-np.sum(Y * np.log(P)+(1 - Y) * np.log(1 - P)))\r\n",
                "cross_entropy([1,1,0], [0.8, 0.7,0.1])     # 0.6851790109107685\r\n",
                "cross_entropy([1., 0., 1., 1.], [0.4, 0.6, 0.1, 0.5])     # 4.828313737302301"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "[1. 1. 0.] [0.8 0.7 0.1]\n",
                        "[1. 0. 1. 1.] [0.4 0.6 0.1 0.5]\n"
                    ]
                },
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "4.828313737302301"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 22
                }
            ],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}